{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9524ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1717ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Regular Imports \n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from inference.distribution import Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1b8b4",
   "metadata": {},
   "source": [
    "# Distributions\n",
    "\n",
    "## Target Distribution $\\phi^4$\n",
    "We load premade samples from a `torch.Tensor` of shape `(batch_size, lattice_points, lattice_points)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fdf7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.distribution import Sampleable\n",
    "class EmpiricalPhi4(Sampleable):\n",
    "\n",
    "    def __init__(self, samples: torch.Tensor, device = None):\n",
    "        self._samples = samples.to(device)\n",
    "        self._lattice_size = int(samples.shape[-1])\n",
    "        \n",
    "    @property\n",
    "    def dim(self):\n",
    "        shape = self._samples.shape\n",
    "        return shape[-1] * shape[-2]\n",
    "\n",
    "    def sample(self, num_samples: int):\n",
    "        '''\n",
    "        Returns\n",
    "            Shape (num_samples, L, L)\n",
    "        '''\n",
    "        batch_size = self._samples.shape[0]\n",
    "        if num_samples > batch_size:\n",
    "            raise ValueError(f\"num_samples ({num_samples}) cannot exceed batch_size ({batch_size})\")\n",
    "\n",
    "        indices = torch.randperm(batch_size)[:num_samples]\n",
    "        return self._samples[indices]\n",
    "    \n",
    "\n",
    "# Load Phi4 Samples\n",
    "samples = torch.load('phi4_coupling0p02_kinetic0p3.pt', map_location=torch.device('cpu')) # Shape (batch_size, L, L)\n",
    "dist_phi4 = EmpiricalPhi4(samples=samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147e704",
   "metadata": {},
   "source": [
    "## Easy to Sample Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612ba00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(Sampleable):\n",
    "\n",
    "    def __init__(self, lattice_size, device = None):\n",
    "        self._lattice_size = lattice_size\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._lattice_size ** 2\n",
    "    \n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "        return torch.rand(num_samples, self._lattice_size, self._lattice_size, device=self.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8958a",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5114cd2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6d0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Type\n",
    "\n",
    "def build_mlp(dims: List[int], activation: Type[torch.nn.Module] = torch.nn.SiLU):\n",
    "        mlp = []\n",
    "        for idx in range(len(dims) - 1):\n",
    "            mlp.append(torch.nn.Linear(dims[idx], dims[idx + 1]))\n",
    "            if idx < len(dims) - 2:\n",
    "                mlp.append(activation())\n",
    "        return torch.nn.Sequential(*mlp)\n",
    "\n",
    "class MLPVectorField(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    MLP-parameterization of the learned vector field u_t^theta(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, hiddens: List[int]):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.net = build_mlp([dim+1] + hiddens + [dim])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: (bs, L, L)\n",
    "        - t: (bs, 1, 1)\n",
    "        Returns:\n",
    "        - u_t^theta(x): (bs, L, L)\n",
    "\n",
    "        \"\"\"\n",
    "        og_shape = x.shape # (bs, L, L)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)  # shape: (bs, L*L)\n",
    "        t = t.view(-1, 1)           # shape: (bs, 1)\n",
    "        xt = torch.cat([x, t], dim=-1)  # shape: (bs, L*L + 1)\n",
    "        xt = self.net(xt)\n",
    "\n",
    "        return xt.view(og_shape)        # reshape to (bs, L, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd8de62",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8bb6e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/var/folders/wd/zr_by8q96891mxc_506sm9sr0000gn/T/ipykernel_36991/960032348.py:16: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  '''\n"
     ]
    }
   ],
   "source": [
    "from inference.sde import SDE, EulerMaruyamaSimulator\n",
    "from learning.train import Trainer\n",
    "from inference.path import ConditionalProbabilityPath\n",
    "\n",
    "\n",
    "class LangevinDynamics(SDE):\n",
    "    ''' \n",
    "    Implements\n",
    "\n",
    "    dX_t = (sigma_t^2 / 2) s(x) dt + sigma_t dW_t\n",
    "    \n",
    "    where s(x) = nabla_x log p(x)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, score, noise_scheduler: torch.Tensor):\n",
    "        '''\n",
    "        The score is of the form `\\nabla_x \\log p(x)`\n",
    "\n",
    "        The limiting distribution of LangevinDynamics is `p(x)`\n",
    "\n",
    "        Args:\n",
    "            score (func): Inputs tensor of shape (batch_size, dim), outputs tensor of shape (batch_size, dim).\n",
    "                Can be `distribution.Density.score`.\n",
    "            noise_scheduler: Inputs torch tensor (batch_size, 1), outputs tensor of shape (batch_size, 1)\n",
    "        '''\n",
    "        self.score = score\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "\n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor):\n",
    "        '''\n",
    "        Args:\n",
    "            xt: (batch_size, L, L)\n",
    "            t: (batch_size, 1, 1)\n",
    "        Returns:\n",
    "            drift: (batch_size, L, L)\n",
    "        '''\n",
    "        _, dim, dim = xt.shape\n",
    "        # noise = self.noise_scheduler(t).repeat(1, dim, dim)\n",
    "\n",
    "        t = t.repeat(xt.shape[0], 1, 1)\n",
    "        return self.score(xt, t)\n",
    "    \n",
    "    def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor):\n",
    "        _, dim, dim = xt.shape\n",
    "        noise = self.noise_scheduler(t).repeat(1, dim)\n",
    "\n",
    "        return noise\n",
    "\n",
    "class ConditionalFlowMatchingTrainer(Trainer):\n",
    "    def __init__(self, \n",
    "                 path: ConditionalProbabilityPath, \n",
    "                 model: MLPVectorField,\n",
    "                 **kwargs):\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.path = path\n",
    "\n",
    "    def get_train_loss(self, batch_size: int) -> torch.Tensor:\n",
    "        z = self.path.p_data.sample(batch_size)\n",
    "        t = torch.rand(batch_size, 1, 1) # (batch_size, 1, 1)\n",
    "        x = self.path.sample_conditional_path(z,t) # (batch_size, lattice_size, lattice_size)\n",
    "\n",
    "        u_model = self.model(x,t)\n",
    "        u_ref = self.path.conditional_vector_field(x,z,t)\n",
    "\n",
    "        return torch.norm(u_model - u_ref) / batch_size\n",
    "    \n",
    "    def inference(self, \n",
    "                  num_samples: int,\n",
    "                  ts: torch.Tensor,\n",
    "                  use_tqdm: bool = False\n",
    "                  ):\n",
    "        sde = LangevinDynamics(score=self.model, noise_scheduler=lambda t: torch.ones_like(t))\n",
    "        simulator= EulerMaruyamaSimulator(sde)\n",
    "        \n",
    "        x_init = self.path.p_data.sample(num_samples)\n",
    "        x_final = simulator.simulate(x=x_init, ts=ts, use_tqdm=use_tqdm)\n",
    "\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33ec2201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61, loss: 0.6544201374053955: : 62it [00:06,  9.24it/s]\n",
      "/var/folders/wd/zr_by8q96891mxc_506sm9sr0000gn/T/ipykernel_36991/960032348.py:16: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  '''\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Construct trainer\u001b[39;00m\n\u001b[32m     13\u001b[39m trainer = ConditionalFlowMatchingTrainer(path, linear_flow_model)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m losses = trainer.train(num_epochs=\u001b[32m10000\u001b[39m, device=device, lr=\u001b[32m1e-5\u001b[39m, batch_size=\u001b[32m8000\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nyu/flows/learning/train.py:34\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, num_epochs, device, lr, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m     33\u001b[39m     opt.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.get_train_loss(**kwargs)\n\u001b[32m     35\u001b[39m     loss.backward()\n\u001b[32m     36\u001b[39m     opt.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mConditionalFlowMatchingTrainer.get_train_loss\u001b[39m\u001b[34m(self, batch_size)\u001b[39m\n\u001b[32m     59\u001b[39m t = torch.rand(batch_size, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m) \u001b[38;5;66;03m# (batch_size, 1, 1)\u001b[39;00m\n\u001b[32m     60\u001b[39m x = \u001b[38;5;28mself\u001b[39m.path.sample_conditional_path(z,t) \u001b[38;5;66;03m# (batch_size, lattice_size, lattice_size)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m u_model = \u001b[38;5;28mself\u001b[39m.model(x,t)\n\u001b[32m     63\u001b[39m u_ref = \u001b[38;5;28mself\u001b[39m.path.conditional_vector_field(x,z,t)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.norm(u_model - u_ref) / batch_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mMLPVectorField.forward\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     32\u001b[39m t = t.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)           \u001b[38;5;66;03m# shape: (bs, 1)\u001b[39;00m\n\u001b[32m     33\u001b[39m xt = torch.cat([x, t], dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# shape: (bs, L*L + 1)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m xt = \u001b[38;5;28mself\u001b[39m.net(xt)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xt.view(og_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = module(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from inference.path import LinearConditionalProbabilityPath\n",
    "\n",
    "# Construct conditional probability path\n",
    "path = LinearConditionalProbabilityPath(\n",
    "    p_simple = GaussianNoise(lattice_size=dist_phi4._lattice_size),\n",
    "    p_data = dist_phi4\n",
    ").to(device)\n",
    "\n",
    "# Construct learnable vector field\n",
    "linear_flow_model = MLPVectorField(dim=dist_phi4.dim, hiddens=[128,128,128,128,128])\n",
    "\n",
    "# Construct trainer\n",
    "trainer = ConditionalFlowMatchingTrainer(path, linear_flow_model)\n",
    "\n",
    "losses = trainer.train(num_epochs=10000, device=device, lr=1e-5, batch_size=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6cab0b",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a019e28c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Simulator.simulate() got an unexpected keyword argument 'use_tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m num_samples = \u001b[32m100\u001b[39m\n\u001b[32m      3\u001b[39m ts = torch.linspace(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1000\u001b[39m).unsqueeze(-\u001b[32m1\u001b[39m).unsqueeze(-\u001b[32m1\u001b[39m).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m samples = trainer.inference(num_samples=num_samples, ts=ts, use_tqdm=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Plot some samples\u001b[39;00m\n\u001b[32m      7\u001b[39m fig, axes = plt.subplots(\u001b[32m2\u001b[39m, \u001b[32m5\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mConditionalFlowMatchingTrainer.inference\u001b[39m\u001b[34m(self, num_samples, ts, use_tqdm)\u001b[39m\n\u001b[32m     73\u001b[39m simulator= EulerMaruyamaSimulator(sde)\n\u001b[32m     75\u001b[39m x_init = \u001b[38;5;28mself\u001b[39m.path.p_data.sample(num_samples)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m x_final = simulator.simulate(x=x_init, ts=ts, use_tqdm=use_tqdm)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x_final\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/torch/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[31mTypeError\u001b[39m: Simulator.simulate() got an unexpected keyword argument 'use_tqdm'"
     ]
    }
   ],
   "source": [
    "num_samples = 100\n",
    "\n",
    "ts = torch.linspace(0, 1, 1000).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "samples = trainer.inference(num_samples=num_samples, ts=ts, use_tqdm=True)\n",
    "\n",
    "# Plot some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(samples[i].cpu().detach())\n",
    "    axes[0, i].set_title(f'Sample {i}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot corresponding target samples for comparison\n",
    "    target_sample = dist_phi4.sample(1)[0]\n",
    "    axes[1, i].imshow(target_sample.cpu().detach())\n",
    "    axes[1, i].set_title(f'Target {i}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2e54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
