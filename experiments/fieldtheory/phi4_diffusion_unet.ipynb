{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Regular Imports \n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from inference.distribution import Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions\n",
    "## Target Distribution $\\phi^4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.distribution import Sampleable\n",
    "class EmpiricalPhi4(Sampleable):\n",
    "\n",
    "    def __init__(self, samples: torch.Tensor, device = None):\n",
    "        self._samples = samples.to(device)\n",
    "        self._lattice_size = int(samples.shape[-1])\n",
    "        \n",
    "    @property\n",
    "    def dim(self):\n",
    "        shape = self._samples.shape\n",
    "        return shape[-1] * shape[-2]\n",
    "\n",
    "    def sample(self, num_samples: int):\n",
    "        '''\n",
    "        Returns\n",
    "            Shape (num_samples, L, L)\n",
    "        '''\n",
    "        batch_size = self._samples.shape[0]\n",
    "        if num_samples > batch_size:\n",
    "            raise ValueError(f\"num_samples ({num_samples}) cannot exceed batch_size ({batch_size})\")\n",
    "\n",
    "        indices = torch.randperm(batch_size)[:num_samples]\n",
    "        return self._samples[indices]\n",
    "    \n",
    "\n",
    "# Load Phi4 Samples\n",
    "samples = torch.load('phi4_coupling0p02_kinetic0p3.pt', map_location=torch.device('cpu')) # Shape (batch_size, L, L)\n",
    "dist_phi4 = EmpiricalPhi4(samples=samples, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy to Sample Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(Sampleable):\n",
    "\n",
    "    def __init__(self, lattice_size, device = None):\n",
    "        self._lattice_size = lattice_size\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._lattice_size ** 2\n",
    "    \n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "        return torch.rand(num_samples, self._lattice_size, self._lattice_size, device=self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.sde import SDE, EulerMaruyamaSimulator\n",
    "from inference.path import ConditionalProbabilityPath\n",
    "from learning.train import Trainer\n",
    "from learning.mlp import MLPVectorField\n",
    "\n",
    "class LangevinDynamics(SDE):\n",
    "    ''' \n",
    "    Implements\n",
    "\n",
    "    dX_t = (sigma_t^2 / 2) s(x) dt + sigma_t dW_t\n",
    "    \n",
    "    where s(x) = nabla_x log p(x)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, score, noise_scheduler: torch.Tensor):\n",
    "        '''\n",
    "        The score is of the form `nabla_x log p(x)`\n",
    "\n",
    "        The limiting distribution of LangevinDynamics is `p(x)`\n",
    "\n",
    "        Args:\n",
    "            score (func): Inputs tensor of shape (batch_size, dim), outputs tensor of shape (batch_size, dim).\n",
    "                Can be `distribution.Density.score`.\n",
    "            noise_scheduler: Inputs torch tensor (batch_size, 1), outputs tensor of shape (batch_size, 1)\n",
    "        '''\n",
    "        self.score = score\n",
    "        self.noise_scheduler = noise_scheduler\n",
    "\n",
    "    def drift_coefficient(self, xt: torch.Tensor, t: torch.Tensor):\n",
    "        '''\n",
    "        Args:\n",
    "            xt: (batch_size, L, L)\n",
    "            t: (batch_size, 1, 1)\n",
    "        Returns:\n",
    "            drift: (batch_size, L, L)\n",
    "        '''\n",
    "        _, dim, dim = xt.shape\n",
    "        # noise = self.noise_scheduler(t).repeat(1, dim, dim)\n",
    "\n",
    "        t = t.repeat(xt.shape[0], 1, 1)\n",
    "        return self.score(xt, t)\n",
    "    \n",
    "    def diffusion_coefficient(self, xt: torch.Tensor, t: torch.Tensor):\n",
    "        _, dim, dim = xt.shape\n",
    "        noise = self.noise_scheduler(t).repeat(1, dim)\n",
    "\n",
    "        return noise\n",
    "\n",
    "class ConditionalFlowMatchingTrainer(Trainer):\n",
    "    def __init__(self, \n",
    "                 path: ConditionalProbabilityPath, \n",
    "                 model: MLPVectorField,\n",
    "                 wandb = None,\n",
    "                 **kwargs):\n",
    "        super().__init__(model, **kwargs)\n",
    "        self.path = path\n",
    "\n",
    "    def get_train_loss(self, batch_size: int) -> torch.Tensor:\n",
    "        z = self.path.p_data.sample(batch_size)\n",
    "        t = torch.rand(batch_size, 1, 1, 1, device=device) # (batch_size, 1, 1)\n",
    "        x = self.path.sample_conditional_path(z,t) # (batch_size, lattice_size, lattice_size)\n",
    "\n",
    "        u_model = self.model(x,t)\n",
    "        u_ref = self.path.conditional_vector_field(x,z,t)\n",
    "\n",
    "        return torch.norm(u_model - u_ref) / batch_size\n",
    "    \n",
    "    def inference(self, \n",
    "                  num_samples: int,\n",
    "                  ts: torch.Tensor,\n",
    "                  use_tqdm: bool = False\n",
    "                  ):\n",
    "        sde = LangevinDynamics(score=self.model, noise_scheduler=lambda t: torch.ones_like(t))\n",
    "        simulator= EulerMaruyamaSimulator(sde)\n",
    "        \n",
    "        x_init = self.path.p_data.sample(num_samples)\n",
    "        x_final = simulator.simulate(x=x_init, ts=ts, use_tqdm=use_tqdm)\n",
    "\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.unet import UNet\n",
    "from inference.path import LinearConditionalProbabilityPath\n",
    "\n",
    "# Construct conditional probability path\n",
    "path = LinearConditionalProbabilityPath(\n",
    "    p_simple = GaussianNoise(lattice_size=dist_phi4._lattice_size, device=device),\n",
    "    p_data = dist_phi4\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "config = dict(  \n",
    "    model = dict(\n",
    "        channels = [1, 4],\n",
    "        num_residual_layers = 2,\n",
    "        t_embed_dim = 64,\n",
    "    ),\n",
    "    setup_optimizer = dict(\n",
    "        lr = 1e-5,\n",
    "        weight_decay = 1e-4\n",
    "    ),\n",
    "    setup_loss = dict(\n",
    "        batch_size = 8000)\n",
    ")\n",
    "\n",
    "# Construct learnable vector field\n",
    "linear_flow_model = UNet(channels=config['model']['channels'],\n",
    "                         num_residual_layers=config['model']['num_residual_layers'],\n",
    "                         t_embed_dim=config['model']['t_embed_dim'])\n",
    "\n",
    "trainer = ConditionalFlowMatchingTrainer(path, linear_flow_model, wandb_record=False)\n",
    "losses = trainer.train(num_epochs=10000, \n",
    "                       device=device, \n",
    "                       setup_optimizer=config['setup_optimizer'], \n",
    "                       setup_loss=config['setup_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "ts = torch.linspace(0, 1, 1000).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "samples = trainer.inference(num_samples=num_samples, ts=ts, use_tqdm=True)\n",
    "\n",
    "# Plot some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(samples[i].cpu().detach())\n",
    "    axes[0, i].set_title(f'Sample {i}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot corresponding target samples for comparison\n",
    "    target_sample = dist_phi4.sample(1)[0]\n",
    "    axes[1, i].imshow(target_sample.cpu().detach())\n",
    "    axes[1, i].set_title(f'Target {i}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Add row titles\n",
    "fig.text(0.5, 0.92, 'Generative Model', ha='center', va='center', fontsize=14, weight='bold')\n",
    "fig.text(0.5, 0.46, 'Target Distribution', ha='center', va='center', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # leave room at the top for the row titles\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from fieldstatistics import compare_statistics\n",
    "\n",
    "num_samples = 5000\n",
    "compare_statistics(samples, dist_phi4.sample(num_samples))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
